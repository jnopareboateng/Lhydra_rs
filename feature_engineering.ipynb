{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# synthetic minority oversampling technique for imbalanced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/music_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for dataframes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\utils\\_indexing.py:338\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m\n\u001b[0;32m     52\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     53\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     54\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrfe\u001b[39m\u001b[38;5;124m'\u001b[39m, RFE(estimator\u001b[38;5;241m=\u001b[39mRandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m), n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)),\n\u001b[0;32m     55\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m     56\u001b[0m ])\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m     62\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:472\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    471\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 472\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:409\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    407\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:1329\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, columns, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1329\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1331\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1332\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1333\u001b[0m         )\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:969\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m    967\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 969\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:536\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    534\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    535\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 536\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32md:\\DEV WORK\\Data Science Library\\ML-For-Beginners\\.venv\\lib\\site-packages\\sklearn\\utils\\_indexing.py:340\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    338\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[1;31mValueError\u001b[0m: Specifying the columns using strings is only supported for dataframes."
     ]
    }
   ],
   "source": [
    "\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = [\"education\", \"gender\", \"country\", \"explicit\"]\n",
    "text_features = ['music','artist_name', 'featured_artists']\n",
    "\n",
    "\n",
    "# Remove the target variable from numerical features\n",
    "numerical_cols.remove('ratings')  # Assuming 'plays' is the target variable\n",
    "\n",
    "\n",
    "# Define preprocessing for features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer(max_features=500)),\n",
    "    ('svd', TruncatedSVD(n_components=80))  # Reducing dimensions\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        # ('music_tfidf', text_transformer,'music'),\n",
    "        # ('artist_tfidf', text_transformer, 'artist_name'),\n",
    "        # ('featured_artists_tfidf', text_transformer, 'featured_artists')\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "X = df.drop([\"music\",\"ratings\"], axis=1)\n",
    "y = df['ratings']\n",
    "\n",
    "# Apply preprocessing to features\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rfe', RFE(estimator=RandomForestRegressor(n_estimators=500, random_state=42), n_features_to_select=10)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=500, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "error = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {error}')\n",
    "\n",
    "\n",
    "feature_importances = pipeline.named_steps['rf'].feature_importances_\n",
    "\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importances_df)), feature_importances_df['Importance'])\n",
    "plt.yticks(range(len(feature_importances_df)), feature_importances_df['Feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238734</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.043934</td>\n",
       "      <td>0.616826</td>\n",
       "      <td>0.601308</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.052688</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.263706</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.030009</td>\n",
       "      <td>0.147427</td>\n",
       "      <td>0.838854</td>\n",
       "      <td>0.807370</td>\n",
       "      <td>0.141007</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.957183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447297</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.168433</td>\n",
       "      <td>0.651925</td>\n",
       "      <td>0.850641</td>\n",
       "      <td>0.930854</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.613005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015971</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.351175</td>\n",
       "      <td>0.790788</td>\n",
       "      <td>0.706059</td>\n",
       "      <td>0.439479</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.947426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086525</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.244357</td>\n",
       "      <td>0.527193</td>\n",
       "      <td>0.272543</td>\n",
       "      <td>0.622771</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.942431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.238734  0.313433  0.001911  0.043934  0.616826  0.601308  0.905902   \n",
       "1  0.263706  0.343284  0.030009  0.147427  0.838854  0.807370  0.141007   \n",
       "2  0.447297  0.014925  0.001366  0.168433  0.651925  0.850641  0.930854   \n",
       "3  0.015971  0.686567  0.001494  0.351175  0.790788  0.706059  0.439479   \n",
       "4  0.086525  0.089552  0.000772  0.244357  0.527193  0.272543  0.622771   \n",
       "\n",
       "         7         8    9   ...   19   20   21   22   23   24   25   26   27  \\\n",
       "0  0.363636  0.052688  1.0  ...  0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0   \n",
       "1  0.272727  0.957183  0.0  ...  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   \n",
       "2  0.454545  0.613005  0.0  ...  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   \n",
       "3  0.454545  0.947426  1.0  ...  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0   \n",
       "4  0.090909  0.942431  1.0  ...  0.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0   \n",
       "\n",
       "    28  \n",
       "0  0.0  \n",
       "1  1.0  \n",
       "2  1.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train)  # Convert numpy array back to DataFrame if needed\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Assuming df is your DataFrame and 'music' is the target variable\n",
    "X = df.drop('music', axis=1)\n",
    "y = df['music']\n",
    "\n",
    "# Identify categorical columns (example method, adjust based on your data)\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Define a transformer that applies OneHotEncoder to the categorical columns and StandardScaler to the numeric columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X.select_dtypes(exclude=['object', 'category']).columns),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply the transformations\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Proceed with PCA on X_transformed\n",
    "pca = PCA().fit(X_transformed)\n",
    "\n",
    "# Plotting the cumulative variance explained\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# # Identify numerical and categorical columns based on dtypes\n",
    "# numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "# categorical_cols = [\"education\", \"gender\", \"country\", \"explicit\"]\n",
    "\n",
    "# # Define the preprocessing for numerical features: scaling and handling skewness\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('variance_threshold', VarianceThreshold(threshold=0.1)),\n",
    "#     ('pca', PCA(n_components=0.95))  # Adjust based on explained variance\n",
    "# ])\n",
    "\n",
    "# # Define the preprocessing for categorical features: encoding\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# # Combine preprocessing steps\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numerical_cols),\n",
    "#         ('cat', categorical_transformer, categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Define the logistic regression model for RFE\n",
    "# logistic_model = LogisticRegression(max_iter=500)  # Increase max_iter for convergence\n",
    "\n",
    "# # Create a pipeline that does preprocessing and RFE, then clustering\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('rfe', RFE(logistic_model, n_features_to_select=10)),  # Adjust n_features_to_select as needed\n",
    "#     ('clusterer', KMeans(n_clusters=10, random_state=42))  # Adjust n_clusters as needed\n",
    "# ])\n",
    "\n",
    "# # Split the data into features and target\n",
    "# X = df.drop('music', axis=1)\n",
    "# y = df['music']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the pipeline on the training data\n",
    "# pipeline.fit(X_train,y_train)\n",
    "\n",
    "# # Function to determine the optimal number of clusters\n",
    "# def optimal_number_of_clusters(X, max_k):\n",
    "#     iters = range(2, max_k+1, 2)\n",
    "#     s = []\n",
    "#     for k in iters:\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "#         s.append(silhouette_score(X, kmeans.labels_))\n",
    "\n",
    "#     f, ax = plt.subplots(1, 1)\n",
    "#     ax.plot(iters, s, marker='o')\n",
    "#     ax.set_xlabel('Cluster Centers')\n",
    "#     ax.set_xticks(iters)\n",
    "#     ax.set_xticklabels(iters)\n",
    "#     ax.set_ylabel('Silhouette Score')\n",
    "#     ax.set_title('Silhouette Scores for Different Cluster Centers')\n",
    "#     plt.show()\n",
    "\n",
    "# # Apply preprocessing only to the data for optimal cluster calculation\n",
    "# X_preprocessed = preprocessor.fit_transform(X_train,y_train)\n",
    "# optimal_number_of_clusters(X_preprocessed, 20)  # Adjust max_k as needed\n",
    "\n",
    "# # Re-fit the pipeline with the optimal number of clusters found\n",
    "# optimal_clusters = 10  # Update this based on silhouette score plot\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('rfe', RFE(logistic_model, n_features_to_select=10)),  # Adjust n_features_to_select as needed\n",
    "#     ('clusterer', KMeans(n_clusters=optimal_clusters, random_state=42))\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline on the training data again with optimal clusters\n",
    "# pipeline.fit(X_train,y_train)\n",
    "\n",
    "# # Transform the test data and predict the clusters\n",
    "# test_clusters = pipeline.predict(test_df)\n",
    "\n",
    "# # Evaluate clustering with silhouette score on the test data\n",
    "# X_test_preprocessed = preprocessor.transform(test_df)\n",
    "# test_silhouette_score = silhouette_score(X_test_preprocessed, test_clusters)\n",
    "# print(f'Silhouette Score on Test Data: {test_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
