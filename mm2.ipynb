{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_data(file_path='../data/dataset.csv')\n",
    "PATH = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(PATH, \"data/enriched_synthetic_data.csv\"))\n",
    "# df.drop([\"music_id\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_value():\n",
    "    # show number of missing values as a dataframe by column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "    missing_values = missing_values.sort_values(ascending=False)\n",
    "    missing_values = missing_values.reset_index()\n",
    "    missing_values.columns = [\"Feature\", \"Missing Values\"]\n",
    "    return missing_values\n",
    "\n",
    "\n",
    "check_missing_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill genre missing values with \"Unknown\"\n",
    "df['genre']= df[\"genre\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id             0\n",
       "age                 0\n",
       "gender              0\n",
       "music               0\n",
       "artist_name         0\n",
       "featured_artists    0\n",
       "genre               0\n",
       "plays               0\n",
       "duration            0\n",
       "music_id            0\n",
       "id_artists          0\n",
       "acousticness        0\n",
       "danceability        0\n",
       "energy              0\n",
       "key                 0\n",
       "loudness            0\n",
       "mode                0\n",
       "speechiness         0\n",
       "instrumentalness    0\n",
       "liveness            0\n",
       "valence             0\n",
       "tempo               0\n",
       "time_signature      0\n",
       "explicit            0\n",
       "rating              0\n",
       "age_group           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns have consistent df types\n",
    "df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "df[\"music_id\"] = df[\"music_id\"].astype(str)\n",
    "df[\"music\"] = df[\"music\"].astype(str)\n",
    "df[\"age\"] = df[\"age\"].astype(int)\n",
    "df[\"gender\"] = df[\"gender\"].astype(str)\n",
    "df[\"age_group\"] = df[\"age_group\"].astype(str)\n",
    "df[\"plays\"] = df[\"plays\"].astype(int)\n",
    "df[\"duration\"] = df[\"duration\"].astype(float)\n",
    "df[\"acousticness\"] = df[\"acousticness\"].astype(float)\n",
    "df[\"danceability\"] = df[\"danceability\"].astype(float)\n",
    "df[\"energy\"] = df[\"energy\"].astype(float)\n",
    "df[\"key\"] = df[\"key\"].astype(int)\n",
    "df[\"loudness\"] = df[\"loudness\"].astype(float)\n",
    "df[\"mode\"] = df[\"mode\"].astype(int)\n",
    "df[\"speechiness\"] = df[\"speechiness\"].astype(float)\n",
    "df[\"instrumentalness\"] = df[\"instrumentalness\"].astype(float)\n",
    "df[\"liveness\"] = df[\"liveness\"].astype(float)\n",
    "df[\"valence\"] = df[\"valence\"].astype(float)\n",
    "df[\"tempo\"] = df[\"tempo\"].astype(float)\n",
    "df[\"time_signature\"] = df[\"time_signature\"].astype(float)\n",
    "df[\"explicit\"] = df[\"explicit\"].astype(bool)\n",
    "df[\"rating\"] = df[\"rating\"].astype(float)\n",
    "# df[\"release_year\"] = df[\"release_year\"].astype(int)\n",
    "# df['release_year'] = df['release_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated dataset size: 313\n",
      "Estimated train size: 15979\n",
      "Estimated test dataset size: 3995\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd\n",
    "\n",
    "# df\n",
    "\n",
    "\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=64):\n",
    "    dataframe = dataframe.copy()\n",
    "    # labels = dataframe.pop('target').astype('float64')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n",
    "    # if shuffle:\n",
    "    # ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Convert DataFrame to TensorFlow dataset\n",
    "dataset = df_to_dataset(df, batch_size=64)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset_size = tf.data.experimental.cardinality(dataset)\n",
    "print(f\"Estimated dataset size: {dataset_size.numpy()}\")\n",
    "\n",
    "\n",
    "# Shuffle the dataset with a buffer size of 1000\n",
    "shuffled_dataset = dataset.shuffle(\n",
    "    buffer_size=len(df), seed=42, reshuffle_each_iteration=False\n",
    ")\n",
    "\n",
    "# Split the shuffled dataset (adjust the split ratio as needed)\n",
    "train_size = int(0.8 * len(df))\n",
    "train_dataset = shuffled_dataset.take(train_size)\n",
    "test_dataset = shuffled_dataset.skip(train_size)\n",
    "print(\n",
    "    f\"Estimated train size: {train_size}\\nEstimated test dataset size: {len(df) - train_size}\"\n",
    ")\n",
    "\n",
    "# # Print dataset shapes for debugging\n",
    "# print(\"Train dataset shapes:\")\n",
    "# for batch in cached_train.take(10):\n",
    "#   for key, value in batch.items():\n",
    "#     print(f\"{key}: {tf.shape(value)}\")\n",
    "\n",
    "# print(\"Test dataset shapes:\")\n",
    "# for batch in cached_test.take(10):\n",
    "#   for key, value in batch.items():\n",
    "#     print(f\"{key}: {tf.shape(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate sample: [b'Tequila' b'Altes Kamuffel' b'Wish You Were Here' b'A-Punk' b'Morning'\n",
      " b'Homegrown' b'Take Me To Church' b'99 Red Balloons'\n",
      " b'Young Dumb & Broke' b'Attention' b'1991' b'Evermore' b'Wait'\n",
      " b'How Deep Is Your Love' b'Sex Me (Part I) / Sex Me (Part II)' b'Shining'\n",
      " b'Mad' b\"I'm Into You\" b'Holy Grail' b'Jesus Saves' b'Aloha' b'Party Up'\n",
      " b'Cecilia' b'Timber' b'Way Down We Go' b'Free to Be Me' b'Like A Virgin'\n",
      " b'POWER' b'Poison & Wine' b'505' b'Yuba Diamond'\n",
      " b'Para Sempre (feat. Elin Melgarejo)' b'Limbo' b'Notice' b'Irreplaceable'\n",
      " b'B.Y.O.B.' b'Whispers In The Dark' b'Goodies' b'Evil Woman' b'Domino'\n",
      " b'Awakening' b'Annie' b'Bulls On Parade' b\"She's so High\" b'Salute'\n",
      " b'Bottoms Up' b'Teenage Dream' b'Landslide' b'Hello, My Name Is'\n",
      " b'As You Are' b'I Want a Hippopotamus for Christmas (Hippo the Hero)'\n",
      " b'Money Longer' b\"Ain't Your Mama\" b'Spiderwebs' b'Sorry' b'Firework'\n",
      " b'Mi Primer Amor' b'My Love (feat. Major Lazer, WizKid, Dua Lipa)'\n",
      " b'I Wanna Dance with Somebody (Who Loves Me)' b'One' b'Dirty Mouth'\n",
      " b'Fly Like An Eagle' b\"Livin' Our Love Song\" b\"What's Up?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 18:56:37.207223: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-07-29 18:56:37.490918: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert '('c', 'o', 'u', 'n', 't', 'e', 'r')' to a shape. Found invalid entry 'c' of type '<class 'str'>'. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m user_model \u001b[38;5;241m=\u001b[39m UserModel()\n\u001b[1;32m    184\u001b[0m item_model \u001b[38;5;241m=\u001b[39m ItemModel()\n\u001b[0;32m--> 185\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMultitaskModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m    188\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[41], line 138\u001b[0m, in \u001b[0;36mMultitaskModel.__init__\u001b[0;34m(self, user_model, item_model, candidates)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_model \u001b[38;5;241m=\u001b[39m user_model\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_model \u001b[38;5;241m=\u001b[39m item_model\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_task \u001b[38;5;241m=\u001b[39m tfrs\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mRetrieval(\n\u001b[0;32m--> 138\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m\u001b[43mtfrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFactorizedTopK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplays_prediction_task \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    141\u001b[0m     [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m), tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplays_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError()\n",
      "File \u001b[0;32m~/anaconda3/envs/venv_310/lib/python3.10/site-packages/tensorflow_recommenders/metrics/factorized_top_k.py:79\u001b[0m, in \u001b[0;36mFactorizedTopK.__init__\u001b[0;34m(self, candidates, ks, name)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(candidates, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m     78\u001b[0m   candidates \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 79\u001b[0m       \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorized_top_k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m       \u001b[38;5;241m.\u001b[39mindex_from_dataset(candidates)\n\u001b[1;32m     81\u001b[0m   )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ks \u001b[38;5;241m=\u001b[39m ks\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_candidates \u001b[38;5;241m=\u001b[39m candidates\n",
      "File \u001b[0;32m~/anaconda3/envs/venv_310/lib/python3.10/site-packages/tensorflow_recommenders/layers/factorized_top_k.py:376\u001b[0m, in \u001b[0;36mStreaming.__init__\u001b[0;34m(self, query_model, k, handle_incomplete_batches, num_parallel_calls, sorted_order)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls \u001b[38;5;241m=\u001b[39m num_parallel_calls\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sorted \u001b[38;5;241m=\u001b[39m sorted_order\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcounter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/venv_310/lib/python3.10/site-packages/keras/src/layers/layer.py:522\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, name)\u001b[0m\n\u001b[1;32m    520\u001b[0m initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(initializer)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 522\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Will be added to layer.losses\u001b[39;00m\n\u001b[1;32m    532\u001b[0m variable\u001b[38;5;241m.\u001b[39mregularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(regularizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/venv_310/lib/python3.10/site-packages/keras/src/backend/common/variables.py:161\u001b[0m, in \u001b[0;36mKerasVariable.__init__\u001b[0;34m(self, initializer, shape, dtype, trainable, autocast, aggregation, name)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(initializer):\n\u001b[0;32m--> 161\u001b[0m         shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m         value \u001b[38;5;241m=\u001b[39m initializer(shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/venv_310/lib/python3.10/site-packages/keras/src/backend/common/variables.py:184\u001b[0m, in \u001b[0;36mKerasVariable._validate_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape):\n\u001b[0;32m--> 184\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m shape:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes used to initialize variables must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfully-defined (no `None` dimensions). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for variable path=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/venv_310/lib/python3.10/site-packages/keras/src/backend/common/variables.py:550\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_int_dtype(\u001b[38;5;28mtype\u001b[39m(e)):\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound invalid entry \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m     )\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative dimensions are not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert '('c', 'o', 'u', 'n', 't', 'e', 'r')' to a shape. Found invalid entry 'c' of type '<class 'str'>'. "
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Define embedding dimensions\n",
    "embedding_dimension = 128\n",
    "\n",
    "# Extract unique values for lookup layers and ensure no duplicates\n",
    "unique_user_ids = df[\"user_id\"].astype(str).unique().tolist()\n",
    "unique_ages = df[\"age\"].unique().tolist()\n",
    "unique_genders = df[\"gender\"].unique().tolist()\n",
    "# unique_song_ids = df[\"music_id\"].unique().tolist()\n",
    "unique_music = df[\"music\"].unique().tolist()\n",
    "unique_genres = df[\"genre\"].unique().tolist()\n",
    "unique_artist_names = df[\"artist_name\"].unique().tolist()\n",
    "unique_age_groups = df[\"age_group\"].unique().tolist()\n",
    "\n",
    "\n",
    "# Define user model\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_user_ids, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    len(unique_user_ids) + 1, embedding_dimension\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.age_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.IntegerLookup(vocabulary=unique_ages, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(unique_ages) + 1, embedding_dimension),\n",
    "            ]\n",
    "        )\n",
    "        self.gender_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_genders, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(len(unique_genders) + 1, embedding_dimension),\n",
    "            ]\n",
    "        )\n",
    "        self.age_group_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_age_groups, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    len(unique_age_groups) + 1, embedding_dimension\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat(\n",
    "            [\n",
    "                self.user_embedding(inputs[\"user_id\"]),\n",
    "                self.age_embedding(inputs[\"age\"]),\n",
    "                self.gender_embedding(inputs[\"gender\"]),\n",
    "                self.age_group_embedding(inputs[\"age_group\"]),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "\n",
    "# Define item model with audio features\n",
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.song_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(vocabulary=unique_music, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(unique_music) + 1, embedding_dimension),\n",
    "            ]\n",
    "        )\n",
    "        self.genre_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(vocabulary=unique_genres, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(unique_genres) + 1, embedding_dimension),\n",
    "            ]\n",
    "        )\n",
    "        self.artist_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_artist_names, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    len(unique_artist_names) + 1, embedding_dimension\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.audio_features = tf.keras.layers.Dense(embedding_dimension)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        song_embedding = self.song_embedding(inputs[\"music\"])\n",
    "        genre_embedding = self.genre_embedding(inputs[\"genre\"])\n",
    "        artist_embedding = self.artist_embedding(inputs[\"artist_name\"])\n",
    "\n",
    "        # Check shape of audio features\n",
    "        audio_features = tf.stack(\n",
    "            [\n",
    "                inputs[\"acousticness\"],\n",
    "                inputs[\"danceability\"],\n",
    "                inputs[\"energy\"],\n",
    "                inputs[\"key\"],\n",
    "                inputs[\"loudness\"],\n",
    "                inputs[\"mode\"],\n",
    "                inputs[\"speechiness\"],\n",
    "                inputs[\"instrumentalness\"],\n",
    "                inputs[\"liveness\"],\n",
    "                inputs[\"valence\"],\n",
    "                inputs[\"tempo\"],\n",
    "                inputs[\"time_signature\"],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Print shapes for debugging\n",
    "        print(f\"Audio features shape before Dense layer: {audio_features.shape}\")\n",
    "\n",
    "        audio_features = self.audio_features(audio_features)\n",
    "\n",
    "        return tf.concat(\n",
    "            [song_embedding, genre_embedding, artist_embedding, audio_features], axis=1\n",
    "        )\n",
    "\n",
    "\n",
    "# Define the multitask model\n",
    "class MultitaskModel(tfrs.models.Model):\n",
    "    def __init__(self, user_model, item_model, candidates):\n",
    "        super().__init__()\n",
    "        self.user_model = user_model\n",
    "        self.item_model = item_model\n",
    "        self.retrieval_task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(candidates=candidates)\n",
    "        )\n",
    "        self.plays_prediction_task = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(128, activation=\"relu\"), tf.keras.layers.Dense(1)]\n",
    "        )\n",
    "        self.plays_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def call(self, features):\n",
    "        user_embeddings = self.user_model(features)\n",
    "        item_embeddings = self.item_model(features)\n",
    "        return user_embeddings, item_embeddings\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings, item_embeddings = self(features)\n",
    "        retrieval_loss = self.retrieval_task(\n",
    "            user_embeddings, item_embeddings, compute_metrics=not training\n",
    "        )\n",
    "        plays_predictions = self.rating_prediction_task(\n",
    "            tf.concat([user_embeddings, item_embeddings], axis=1)\n",
    "        )\n",
    "        plays_loss = self.rating_loss(features[\"plays\"], plays_predictions)\n",
    "        return retrieval_loss + plays_loss\n",
    "\n",
    "\n",
    "# # Convert DataFrame to TensorFlow datasets\n",
    "# def df_to_tf_dataset(dataframe):\n",
    "#     return tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "\n",
    "\n",
    "# # Prepare datasets\n",
    "# train_dataset = df_to_tf_dataset(train)\n",
    "# test_dataset = df_to_tf_dataset(test)\n",
    "\n",
    "# Check if candidates are not empty\n",
    "candidates = train_dataset.map(lambda x: x[\"music\"])\n",
    "for candidate in candidates.take(1):\n",
    "    print(f\"Candidate sample: {candidate.numpy()}\")\n",
    "\n",
    "# Ensure candidates are correctly mapped\n",
    "if not list(candidates):\n",
    "    raise ValueError(\n",
    "        \"Candidates set is empty. Ensure 'music' is correctly mapped from the dataset.\"\n",
    "    )\n",
    "\n",
    "# Instantiate models\n",
    "user_model = UserModel()\n",
    "item_model = ItemModel()\n",
    "model = MultitaskModel(user_model, item_model, candidates)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Cache datasets for performance\n",
    "batch_size = 1024  # Adjust as needed\n",
    "\n",
    "cached_train = train_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "cached_test = test_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Print dataset shapes for debugging\n",
    "print(\"Train dataset shapes:\")\n",
    "for batch in cached_train.take(1):\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {tf.shape(value)}\")\n",
    "\n",
    "print(\"Test dataset shapes:\")\n",
    "for batch in cached_test.take(1):\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {tf.shape(value)}\")\n",
    "\n",
    "# Fit the model\n",
    "try:\n",
    "    model.fit(cached_train, epochs=10)\n",
    "except Exception as e:\n",
    "    print(f\"Error during model fitting: {e}\")\n",
    "\n",
    "# Evaluate the model\n",
    "try:\n",
    "    model.evaluate(cached_test, return_dict=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error during model evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
