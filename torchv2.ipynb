{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, test_size=0.2, random_state=42):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.user_id_encoder = LabelEncoder()\n",
    "        self.music_id_encoder = LabelEncoder()\n",
    "        self.gender_encoder = LabelEncoder()\n",
    "        self.artist_tfidf_vectorizer = TfidfVectorizer()\n",
    "        self.genre_tfidf_vectorizer = TfidfVectorizer()\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the CSV file.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded data.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        return data\n",
    "    \n",
    "    def encode_features(self, data):\n",
    "        \"\"\"\n",
    "        Encode categorical features using LabelEncoder and TF-IDF Vectorizer.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Data to encode.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Data with encoded features.\n",
    "        \"\"\"\n",
    "        # Label Encoding\n",
    "        data['user_id_encoded'] = self.user_id_encoder.fit_transform(data['user_id'])\n",
    "        data['music_id_encoded'] = self.music_id_encoder.fit_transform(data['music_id'])\n",
    "        data['gender_encoded'] = self.gender_encoder.fit_transform(data['gender'])\n",
    "        \n",
    "        # TF-IDF Encoding for 'artist_name' and 'genre'\n",
    "        artist_tfidf = self.artist_tfidf_vectorizer.fit_transform(data['artist_name'])\n",
    "        genre_tfidf = self.genre_tfidf_vectorizer.fit_transform(data['genre'])\n",
    "        \n",
    "        # Convert TF-IDF matrices to DataFrames\n",
    "        artist_tfidf_df = pd.DataFrame(artist_tfidf.toarray(), columns=[f'artist_tfidf_{i}' for i in range(artist_tfidf.shape[1])])\n",
    "        genre_tfidf_df = pd.DataFrame(genre_tfidf.toarray(), columns=[f'genre_tfidf_{i}' for i in range(genre_tfidf.shape[1])])\n",
    "        \n",
    "        # Concatenate encoded DataFrames with the original DataFrame\n",
    "        data_encoded = pd.concat([\n",
    "            data[['user_id_encoded', 'music_id_encoded', 'age', 'gender_encoded', 'duration', 'acousticness', \n",
    "                  'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'instrumentalness', \n",
    "                  'liveness', 'valence', 'tempo', 'time_signature', 'explicit', 'plays']],\n",
    "            artist_tfidf_df,\n",
    "            genre_tfidf_df\n",
    "        ], axis=1)\n",
    "        \n",
    "        return data_encoded\n",
    "    \n",
    "    def feature_engineering(self, data_encoded):\n",
    "        \"\"\"\n",
    "        Perform feature scaling on numerical features.\n",
    "        \n",
    "        Args:\n",
    "            data_encoded (pd.DataFrame): Encoded data.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Data with scaled numerical features.\n",
    "        \"\"\"\n",
    "        numerical_features = ['age', 'duration', 'acousticness', 'danceability', 'energy', 'key', 'loudness', \n",
    "                              'mode', 'speechiness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                              'time_signature', 'explicit']\n",
    "        data_encoded[numerical_features] = self.scaler.fit_transform(data_encoded[numerical_features])\n",
    "        return data_encoded\n",
    "    \n",
    "    def split_data(self, data_encoded, target_column='plays'):\n",
    "        \"\"\"\n",
    "        Split data into training and testing sets.\n",
    "        \n",
    "        Args:\n",
    "            data_encoded (pd.DataFrame): Encoded and scaled data.\n",
    "            target_column (str): Name of the target column.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: Train features, test features, train target, test target.\n",
    "        \"\"\"\n",
    "        features = data_encoded.drop(columns=[target_column])\n",
    "        target = data_encoded[target_column]\n",
    "        train_features, test_features, train_target, test_target = train_test_split(\n",
    "            features,\n",
    "            target,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return train_features, test_features, train_target, test_target\n",
    "    \n",
    "    def save_preprocessors(self, directory='models/'):\n",
    "        \"\"\"\n",
    "        Save encoders and vectorizers to disk.\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Directory where the models will be saved.\n",
    "        \"\"\"\n",
    "        with open(f'{directory}user_id_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.user_id_encoder, f)\n",
    "        \n",
    "        with open(f'{directory}music_id_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.music_id_encoder, f)\n",
    "        \n",
    "        with open(f'{directory}gender_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.gender_encoder, f)\n",
    "        \n",
    "        with open(f'{directory}artist_tfidf_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.artist_tfidf_vectorizer, f)\n",
    "        \n",
    "        with open(f'{directory}genre_tfidf_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.genre_tfidf_vectorizer, f)\n",
    "        \n",
    "        with open(f'{directory}scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "    \n",
    "    def load_preprocessors(self, directory='models/'):\n",
    "        \"\"\"\n",
    "        Load encoders and vectorizers from disk.\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Directory where the models are saved.\n",
    "        \"\"\"\n",
    "        with open(f'{directory}user_id_encoder.pkl', 'rb') as f:\n",
    "            self.user_id_encoder = pickle.load(f)\n",
    "        \n",
    "        with open(f'{directory}music_id_encoder.pkl', 'rb') as f:\n",
    "            self.music_id_encoder = pickle.load(f)\n",
    "        \n",
    "        with open(f'{directory}gender_encoder.pkl', 'rb') as f:\n",
    "            self.gender_encoder = pickle.load(f)\n",
    "        \n",
    "        with open(f'{directory}artist_tfidf_vectorizer.pkl', 'rb') as f:\n",
    "            self.artist_tfidf_vectorizer = pickle.load(f)\n",
    "        \n",
    "        with open(f'{directory}genre_tfidf_vectorizer.pkl', 'rb') as f:\n",
    "            self.genre_tfidf_vectorizer = pickle.load(f)\n",
    "        \n",
    "        with open(f'{directory}scaler.pkl', 'rb') as f:\n",
    "            self.scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ListNetLoss(nn.Module):\n",
    "    def __init__(self, k=10):\n",
    "        super(ListNetLoss, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        y_true = F.softmax(y_true, dim=1)\n",
    "        return -torch.sum(y_true * torch.log(y_pred + 1e-10), dim=1).mean()\n",
    "\n",
    "class HybridRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, num_features, num_layers, hidden_dims, dropout_prob):\n",
    "        super(HybridRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        layers = []\n",
    "        input_dim = embedding_dim * 2 + num_features\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_dims[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_prob))\n",
    "            input_dim = hidden_dims[i]\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, features):\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        item_embedded = self.item_embedding(item_ids)\n",
    "        concat_features = torch.cat((user_embedded, item_embedded, features), dim=1)\n",
    "        output = self.fc_layers(concat_features)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPreprocessor\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HybridRecommender, ListNetLoss\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from preprocessing import DataPreprocessor\n",
    "from tensorflow_docs.model import HybridRecommender, ListNetLoss\n",
    "import pickle\n",
    "\n",
    "def main():\n",
    "    # Initialize the preprocessor\n",
    "    preprocessor = DataPreprocessor(test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data_filepath = 'data/raw_data.csv'  # Update with your actual data path\n",
    "    data = preprocessor.load_data(data_filepath)\n",
    "    data_encoded = preprocessor.encode_features(data)\n",
    "    data_encoded = preprocessor.feature_engineering(data_encoded)\n",
    "    train_features, test_features, train_target, test_target = preprocessor.split_data(data_encoded, target_column='plays')\n",
    "    \n",
    "    # Save preprocessors\n",
    "    preprocessor.save_preprocessors(directory='models/')\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    train_data_tensor = torch.tensor(train_features.values, dtype=torch.float)\n",
    "    test_data_tensor = torch.tensor(test_features.values, dtype=torch.float)\n",
    "    train_target_tensor = torch.tensor(train_target.values, dtype=torch.float).unsqueeze(1)\n",
    "    test_target_tensor = torch.tensor(test_target.values, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Extract user IDs, item IDs, and feature tensors\n",
    "    train_user_ids = train_data_tensor[:, 0].long()\n",
    "    train_item_ids = train_data_tensor[:, 1].long()\n",
    "    train_features = train_data_tensor[:, 2:]\n",
    "    \n",
    "    test_user_ids = test_data_tensor[:, 0].long()\n",
    "    test_item_ids = test_data_tensor[:, 1].long()\n",
    "    test_features = test_data_tensor[:, 2:]\n",
    "    \n",
    "    # Initialize the model\n",
    "    num_users = len(preprocessor.user_id_encoder.classes_)\n",
    "    num_items = len(preprocessor.music_id_encoder.classes_)\n",
    "    embedding_dim = 128\n",
    "    num_layers = 3\n",
    "    hidden_dims = [256, 128, 64]\n",
    "    num_features = train_features.shape[1]\n",
    "    dropout_prob = 0.2\n",
    "    \n",
    "    model = HybridRecommender(num_users, num_items, embedding_dim, num_features, num_layers, hidden_dims, dropout_prob)\n",
    "    \n",
    "    # Define optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = ListNetLoss(k=10)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    print(f\"Training on device: {device}\")\n",
    "    \n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(train_user_ids, train_item_ids, train_features, train_target_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_user_ids, batch_item_ids, batch_features, batch_target in dataloader:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "            batch_item_ids = batch_item_ids.to(device)\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_user_ids, batch_item_ids, batch_features)\n",
    "            loss = criterion(predictions.unsqueeze(1), batch_target)\n",
    "            \n",
    "            # Add L2 regularization\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param)\n",
    "            loss += 1e-4 * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        average_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation with NDCG@10\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_user_ids.to(device), test_item_ids.to(device), test_features.to(device))\n",
    "        ndcg_score = ndcg_at_k(test_target_tensor.cpu().numpy(), predictions.cpu().numpy(), k=10)\n",
    "        print(f\"Test NDCG@10: {ndcg_score:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'models/model.pth')\n",
    "    print(\"Model saved to 'models/model.pth'\")\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Discounted Cumulative Gain (NDCG) at k.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): Ground truth scores.\n",
    "        y_pred (np.array): Predicted scores.\n",
    "        k (int): Rank cutoff.\n",
    "        \n",
    "    Returns:\n",
    "        float: NDCG score.\n",
    "    \"\"\"\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    \n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[indices]\n",
    "    \n",
    "    gains = np.log2(y_true_sorted + 1)\n",
    "    discounts = np.log2(np.arange(len(y_true_sorted)) + 2)\n",
    "    dcg = np.sum(gains[:k] / discounts[:k])\n",
    "    \n",
    "    ideal_gains = np.log2(np.sort(y_true)[::-1] + 1)\n",
    "    ideal_dcg = np.sum(ideal_gains[:k] / discounts[:k])\n",
    "    \n",
    "    return dcg / ideal_dcg if ideal_dcg != 0 else 0.0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPreprocessor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HybridRecommender\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "# inference.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from preprocessing import DataPreprocessor\n",
    "from tensorflow_docs.model import HybridRecommender\n",
    "import pickle\n",
    "\n",
    "def make_inference(model, user_id, item_id, features, user_encoder, item_encoder, device):\n",
    "    \"\"\"\n",
    "    Perform inference to predict plays for a specific user-item pair.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained recommender model.\n",
    "        user_id (str): The user ID.\n",
    "        item_id (str): The item (music) ID.\n",
    "        features (pd.Series): Feature vector for the user-item pair.\n",
    "        user_encoder (LabelEncoder): Fitted LabelEncoder for user IDs.\n",
    "        item_encoder (LabelEncoder): Fitted LabelEncoder for item IDs.\n",
    "        device (torch.device): Device where the model is loaded.\n",
    "    \n",
    "    Returns:\n",
    "        float: Predicted number of plays.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    user_id_encoded = torch.tensor(user_encoder.transform([user_id]), dtype=torch.long).to(device)\n",
    "    item_id_encoded = torch.tensor(item_encoder.transform([item_id]), dtype=torch.long).to(device)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(user_id_encoded, item_id_encoded, features_tensor)\n",
    "    return prediction.cpu().numpy()[0]\n",
    "\n",
    "def get_recommendations(model, user_id, data_encoded, user_id_encoder, item_encoder, device, top_k=10):\n",
    "    \"\"\"\n",
    "    Generate top-k recommendations for a given user.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained recommender model.\n",
    "        user_id (str): The user ID for whom to generate recommendations.\n",
    "        data_encoded (pd.DataFrame): The preprocessed and encoded dataset.\n",
    "        user_id_encoder (LabelEncoder): Fitted LabelEncoder for user IDs.\n",
    "        item_encoder (LabelEncoder): Fitted LabelEncoder for item IDs.\n",
    "        device (torch.device): Device where the model is loaded.\n",
    "        top_k (int): Number of top recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of recommended music IDs.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Encode the user ID\n",
    "    try:\n",
    "        user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"User ID {user_id} not found in encoder.\")\n",
    "\n",
    "    # Extract user features\n",
    "    user_data = data_encoded[data_encoded['user_id_encoded'] == user_id_encoded].drop(columns=['user_id_encoded', 'plays']).iloc[0]\n",
    "    user_features = torch.tensor(user_data.values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare all item IDs\n",
    "    item_ids = torch.arange(len(item_encoder.classes_)).to(device)\n",
    "\n",
    "    # Create user and item tensors\n",
    "    user_ids = torch.tensor([user_id_encoded] * len(item_ids), dtype=torch.long).to(device)\n",
    "    item_ids = item_ids.long()\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get scores\n",
    "        scores = model(user_ids, item_ids, user_features.repeat(len(item_ids), 1))\n",
    "\n",
    "    # Get top-k scores and corresponding item IDs\n",
    "    top_scores, top_indices = torch.topk(scores, top_k)\n",
    "    top_item_ids_encoded = top_indices.cpu().numpy()\n",
    "\n",
    "    # Convert encoded item IDs back to original IDs\n",
    "    top_item_ids = item_encoder.inverse_transform(top_item_ids_encoded)\n",
    "\n",
    "    return top_item_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReNeLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
