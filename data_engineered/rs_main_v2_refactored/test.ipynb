{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    RobustScaler,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan:**\n",
    "\n",
    "1. Import SelectKBest and scoring functions\n",
    "2. Add method to DataPreprocessor to analyze features\n",
    "3. Visualize results using matplotlib\n",
    "4. Return top K features with their scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npreprocessor = DataPreprocessor()\\ndata = preprocessor.load_data(\"your_data.csv\")\\n\\n# Check class distribution\\ndistribution = preprocessor.analyze_class_distribution(data[\\'plays\\'])\\n\\n# Balance classes if needed\\nX_resampled, y_resampled = preprocessor.balance_classes(\\n    data_encoded, \\n    data[\\'plays\\'],\\n    method=\\'smote\\'\\n)\\n\\n# Inspect TF-IDF features\\nvocab, features = preprocessor.inspect_tfidf_features(\\'music\\')\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    NUMERICAL_COLS = [\n",
    "        \"age\",\n",
    "        \"duration\",\n",
    "        \"acousticness\",\n",
    "        # \"danceability\",\n",
    "        # \"energy\",\n",
    "        \"key\",\n",
    "        # \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        # \"valence\",\n",
    "        \"tempo\",\n",
    "        \"time_signature\",\n",
    "        \"explicit\",\n",
    "        \"dance_valence\",\n",
    "        \"energy_loudness\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        max_artist_features=3995,\n",
    "        max_genre_features=21,\n",
    "        max_music_features=5784,\n",
    "    ):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.gender_encoder = LabelEncoder()  # Changed to LabelEncoder\n",
    "        self.gender_encoder = LabelEncoder()\n",
    "        self.artist_tfidf_vectorizer = TfidfVectorizer(max_features=max_artist_features)\n",
    "        self.genre_tfidf_vectorizer = TfidfVectorizer(max_features=max_genre_features)\n",
    "        self.music_tfidf_vectorizer = TfidfVectorizer(max_features=max_music_features)\n",
    "        self.release_year_encoder = OrdinalEncoder()  # Added release year encoder\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from a CSV file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the CSV file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded data.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        return data\n",
    "\n",
    "    # def hash_user_id(self, user_id):\n",
    "    #     \"\"\"\n",
    "    #     Convert user_id to 32-dimensional numeric tensor.\n",
    "    #     Returns zero vector for null/empty values.\n",
    "    #     \"\"\"\n",
    "    #     # Handle null/empty values\n",
    "    #     if pd.isna(user_id) or str(user_id).strip() == \"\":\n",
    "    #         return np.zeros(32)\n",
    "\n",
    "    #     # Hash valid user_id\n",
    "    #     hash_hex = hashlib.md5(str(user_id).encode()).hexdigest()\n",
    "\n",
    "    #     # Convert hex string to 32 integers (2 chars per integer)\n",
    "    #     try:\n",
    "    #         return np.array([int(hash_hex[i : i + 2], 16) for i in range(0, 64, 2)])\n",
    "    #     except ValueError:\n",
    "    #         # Return zero vector if conversion fails\n",
    "    #         return np.zeros(32)\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit all preprocessors on training data\"\"\"\n",
    "        self.gender_encoder.fit(data[\"gender\"])\n",
    "        self.release_year_encoder.fit(data[\"release_year\"])\n",
    "        self.artist_tfidf_vectorizer.fit(data[\"artist_id\"])\n",
    "        self.genre_tfidf_vectorizer.fit(data[\"genre\"])\n",
    "        self.music_tfidf_vectorizer.fit(data[\"music_id\"])\n",
    "        self.scaler.fit(data[self.NUMERICAL_COLS])\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, is_training=False):\n",
    "        \"\"\"Transform data using fitted preprocessors\"\"\"\n",
    "        transform_fn = lambda enc, x: (\n",
    "            enc.fit_transform(x) if is_training else enc.transform(x)\n",
    "        )\n",
    "\n",
    "        data_encoded = pd.DataFrame(\n",
    "            {\n",
    "                # \"user_id_hashed\": np.vstack(data[\"user_id\"].apply(self.hash_user_id)),\n",
    "                \"gender_encoded\": transform_fn(self.gender_encoder, data[\"gender\"]),\n",
    "                \"release_year_encoded\": transform_fn(\n",
    "                    self.release_year_encoder, data[\"release_year\"]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add other transformations...\n",
    "        return data_encoded\n",
    "\n",
    "    def encode_features_train(self, data):\n",
    "        # Hash user IDs to 32-dim vectors\n",
    "        # user_id_hashed = np.vstack(data[\"user_id\"].apply(self.hash_user_id))\n",
    "\n",
    "        # Encode gender\n",
    "        data[\"gender_encoded\"] = self.gender_encoder.fit_transform(data[\"gender\"])\n",
    "\n",
    "        # Reshape release_year to 2D array and encode\n",
    "        release_year_2d = data[\"release_year\"].values.reshape(-1, 1)\n",
    "        data[\"release_year_encoded\"] = self.release_year_encoder.fit_transform(\n",
    "            release_year_2d\n",
    "        ).ravel()\n",
    "\n",
    "        # Rest of the method remains the same...\n",
    "        # TF-IDF Encoding\n",
    "        artist_tfidf = self.artist_tfidf_vectorizer.fit_transform(data[\"artist_name\"])\n",
    "        genre_tfidf = self.genre_tfidf_vectorizer.fit_transform(data[\"genre\"])\n",
    "        music_tfidf = self.music_tfidf_vectorizer.fit_transform(data[\"music\"])\n",
    "\n",
    "        # Get actual feature names from vectorizers\n",
    "        artist_feature_names = [\n",
    "            f\"artist_tfidf_{i}\" for i in range(artist_tfidf.shape[1])\n",
    "        ]\n",
    "        genre_feature_names = [f\"genre_tfidf_{i}\" for i in range(genre_tfidf.shape[1])]\n",
    "        music_feature_names = [f\"music_tfidf_{i}\" for i in range(music_tfidf.shape[1])]\n",
    "\n",
    "        # Create DataFrames with actual dimensions\n",
    "        artist_tfidf_df = pd.DataFrame(\n",
    "            artist_tfidf.toarray(), columns=artist_feature_names\n",
    "        )\n",
    "        genre_tfidf_df = pd.DataFrame(\n",
    "            genre_tfidf.toarray(), columns=genre_feature_names\n",
    "        )\n",
    "        music_tfidf_df = pd.DataFrame(\n",
    "            music_tfidf.toarray(), columns=music_feature_names\n",
    "        )\n",
    "\n",
    "        # Combine all features in the expected order\n",
    "        numerical_features = []\n",
    "\n",
    "        data_encoded = pd.DataFrame(\n",
    "            {\n",
    "                \"user_id_hashed\": list(user_id_hashed),\n",
    "                \"gender_encoded\": data[\"gender_encoded\"],\n",
    "                \"release_year_encoded\": data[\"release_year_encoded\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        data_encoded = pd.concat(\n",
    "            [\n",
    "                data_encoded,\n",
    "                music_tfidf_df,\n",
    "                artist_tfidf_df,\n",
    "                genre_tfidf_df,\n",
    "                data[numerical_features],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Verify release_year exists\n",
    "        if \"release_year\" not in data.columns:\n",
    "            raise ValueError(\"release_year column missing from input data\")\n",
    "\n",
    "        # Add release year check after encoding\n",
    "        if \"release_year_encoded\" not in data_encoded.columns:\n",
    "            raise ValueError(\"release_year_encoded not properly created\")\n",
    "\n",
    "        return data_encoded\n",
    "\n",
    "    def encode_features_transform(self, data):\n",
    "        \"\"\"Transform test/inference data.\"\"\"\n",
    "        # Hash user IDs to 32-dim vectors\n",
    "        # user_id_hashed = np.vstack(data[\"user_id\"].apply(self.hash_user_id))\n",
    "\n",
    "        # Encode gender and release year\n",
    "        data[\"gender_encoded\"] = self.gender_encoder.transform(data[\"gender\"])\n",
    "\n",
    "        # Reshape release_year to 2D array and encode\n",
    "        release_year_2d = data[\"release_year\"].values.reshape(-1, 1)\n",
    "        data[\"release_year_encoded\"] = self.release_year_encoder.transform(\n",
    "            release_year_2d\n",
    "        ).ravel()\n",
    "\n",
    "        # TF-IDF Encoding\n",
    "        artist_tfidf = self.artist_tfidf_vectorizer.transform(data[\"artist_name\"])\n",
    "        genre_tfidf = self.genre_tfidf_vectorizer.transform(data[\"genre\"])\n",
    "        music_tfidf = self.music_tfidf_vectorizer.transform(data[\"music\"])\n",
    "\n",
    "        # Get actual feature names from vectorizers\n",
    "        artist_feature_names = [\n",
    "            f\"artist_tfidf_{i}\" for i in range(artist_tfidf.shape[1])\n",
    "        ]\n",
    "        genre_feature_names = [f\"genre_tfidf_{i}\" for i in range(genre_tfidf.shape[1])]\n",
    "        music_feature_names = [f\"music_tfidf_{i}\" for i in range(music_tfidf.shape[1])]\n",
    "\n",
    "        # Create DataFrames with actual dimensions\n",
    "        artist_tfidf_df = pd.DataFrame(\n",
    "            artist_tfidf.toarray(), columns=artist_feature_names\n",
    "        )\n",
    "        genre_tfidf_df = pd.DataFrame(\n",
    "            genre_tfidf.toarray(), columns=genre_feature_names\n",
    "        )\n",
    "        music_tfidf_df = pd.DataFrame(\n",
    "            music_tfidf.toarray(), columns=music_feature_names\n",
    "        )\n",
    "\n",
    "        # Combine all features in the expected order\n",
    "        numerical_features = [\n",
    "            \"age\",\n",
    "            \"duration\",\n",
    "            \"acousticness\",\n",
    "            # \"danceability\",\n",
    "            # \"energy\",\n",
    "            \"key\",\n",
    "            # \"loudness\",\n",
    "            \"mode\",\n",
    "            \"speechiness\",\n",
    "            \"instrumentalness\",\n",
    "            \"liveness\",\n",
    "            # \"valence\",\n",
    "            \"tempo\",\n",
    "            \"time_signature\",\n",
    "            \"explicit\",\n",
    "            \"dance_valence\",\n",
    "            \"energy_loudness\",\n",
    "        ]\n",
    "\n",
    "        data_encoded = pd.DataFrame(\n",
    "            {\n",
    "                # \"user_id_hashed\": list(user_id_hashed),\n",
    "                \"gender_encoded\": data[\"gender_encoded\"],\n",
    "                \"release_year_encoded\": data[\"release_year_encoded\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        data_encoded = pd.concat(\n",
    "            [\n",
    "                data_encoded,\n",
    "                music_tfidf_df,\n",
    "                artist_tfidf_df,\n",
    "                genre_tfidf_df,\n",
    "                data[numerical_features],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Verify release_year exists\n",
    "        if \"release_year\" not in data.columns:\n",
    "            raise ValueError(\"release_year column missing from input data\")\n",
    "\n",
    "        # Add release year check after encoding\n",
    "        if \"release_year_encoded\" not in data_encoded.columns:\n",
    "            raise ValueError(\"release_year_encoded not properly created\")\n",
    "\n",
    "        return data_encoded\n",
    "\n",
    "    def feature_engineering(self, data_encoded):\n",
    "        \"\"\"Scale numerical features and normalize plays\"\"\"\n",
    "        numerical_features = [\n",
    "            \"age\",\n",
    "            \"duration\",\n",
    "            \"acousticness\",\n",
    "            # \"danceability\",\n",
    "            # \"energy\",\n",
    "            \"key\",\n",
    "            # \"loudness\",\n",
    "            \"mode\",\n",
    "            \"speechiness\",\n",
    "            \"instrumentalness\",\n",
    "            \"liveness\",\n",
    "            # \"valence\",\n",
    "            \"tempo\",\n",
    "            \"time_signature\",\n",
    "            \"explicit\",\n",
    "            \"dance_valence\",\n",
    "            \"energy_loudness\",\n",
    "        ]\n",
    "\n",
    "        # Store plays separately before scaling\n",
    "        target = None\n",
    "        if \"plays\" in data_encoded.columns:\n",
    "            target = data_encoded[\"plays\"]\n",
    "            # Add log transformation for skewed play counts\n",
    "            target = np.log1p(target)  # Add 1 and take log\n",
    "            # Then normalize\n",
    "            # target = (target - target.min()) / (target.max() - target.min())\n",
    "\n",
    "        # Scale numerical features\n",
    "        data_encoded[numerical_features] = self.scaler.fit_transform(\n",
    "            data_encoded[numerical_features]\n",
    "        )\n",
    "\n",
    "        # Add normalized plays back\n",
    "        if target is not None:\n",
    "            data_encoded[\"plays\"] = target\n",
    "\n",
    "        return data_encoded\n",
    "\n",
    "    def split_data(self, data_encoded, target_column=\"plays\", val_size=0.1):\n",
    "        features = data_encoded.drop(columns=[target_column])\n",
    "        target = data_encoded[target_column]\n",
    "\n",
    "        # Handle duplicate values in binning\n",
    "        try:\n",
    "            bins = pd.qcut(target, q=10, labels=False, duplicates=\"drop\")\n",
    "        except ValueError:\n",
    "            # Fallback if too many duplicates\n",
    "            bins = pd.cut(target, bins=10, labels=False)\n",
    "\n",
    "        # Split with stratification\n",
    "        train_features, temp_features, train_target, temp_target = train_test_split(\n",
    "            features,\n",
    "            target,\n",
    "            test_size=self.test_size + val_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=bins,\n",
    "        )\n",
    "\n",
    "        # Further split temp into validation and test\n",
    "        val_features, test_features, val_target, test_target = train_test_split(\n",
    "            temp_features,\n",
    "            temp_target,\n",
    "            test_size=self.test_size / (self.test_size + val_size),\n",
    "            random_state=self.random_state,\n",
    "            stratify=pd.qcut(temp_target, q=5, labels=False, duplicates=\"drop\"),\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            train_features,\n",
    "            val_features,\n",
    "            test_features,\n",
    "            train_target,\n",
    "            val_target,\n",
    "            test_target,\n",
    "        )\n",
    "\n",
    "    def save_preprocessors(self, directory=\"models/\"):\n",
    "        \"\"\"Save all preprocessors including release year encoder\"\"\"\n",
    "        preprocessors = {\n",
    "            \"gender_encoder\": self.gender_encoder,\n",
    "            \"artist_tfidf_vectorizer\": self.artist_tfidf_vectorizer,\n",
    "            \"genre_tfidf_vectorizer\": self.genre_tfidf_vectorizer,\n",
    "            \"music_tfidf_vectorizer\": self.music_tfidf_vectorizer,\n",
    "            \"release_year_encoder\": self.release_year_encoder,\n",
    "            \"scaler\": self.scaler,\n",
    "        }\n",
    "\n",
    "        for name, preprocessor in preprocessors.items():\n",
    "            with open(f\"{directory}{name}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(preprocessor, f)\n",
    "\n",
    "    def load_preprocessors(self, directory=\"models/\"):\n",
    "        \"\"\"Load all preprocessors including release year encoder\"\"\"\n",
    "        preprocessors = {\n",
    "            \"gender_encoder\": \"gender_encoder.pkl\",\n",
    "            \"artist_tfidf_vectorizer\": \"artist_tfidf_vectorizer.pkl\",\n",
    "            \"genre_tfidf_vectorizer\": \"genre_tfidf_vectorizer.pkl\",\n",
    "            \"music_tfidf_vectorizer\": \"music_tfidf_vectorizer.pkl\",\n",
    "            \"release_year_encoder\": \"release_year_encoder.pkl\",\n",
    "            \"scaler\": \"scaler.pkl\",\n",
    "        }\n",
    "\n",
    "        for attr, filename in preprocessors.items():\n",
    "            with open(f\"{directory}{filename}\", \"rb\") as f:\n",
    "                setattr(self, attr, pickle.load(f))\n",
    "\n",
    "    # def analyze_feature_importance(self, data_encoded, target, k=10):\n",
    "    #     \"\"\"\n",
    "    #     Analyze and visualize the K most important features using f_regression.\n",
    "\n",
    "    #     Args:\n",
    "    #         data_encoded: DataFrame of encoded features\n",
    "    #         target: Series of target values (plays)\n",
    "    #         k: Number of top features to select\n",
    "    #     \"\"\"\n",
    "    #     # Remove non-numeric columns and target\n",
    "    #     feature_cols = data_encoded.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "    #     X = data_encoded[feature_cols]\n",
    "\n",
    "    #     # Initialize and fit SelectKBest\n",
    "    #     selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    #     selector.fit(X, target)\n",
    "\n",
    "    #     # Get scores and feature names\n",
    "    #     scores = pd.DataFrame(\n",
    "    #         {\"Feature\": feature_cols, \"Score\": selector.scores_}\n",
    "    #     ).sort_values(\"Score\", ascending=False)\n",
    "\n",
    "    #     # Plot top K features\n",
    "    #     plt.figure(figsize=(12, 6))\n",
    "    #     sns.barplot(data=scores.head(k), x=\"Score\", y=\"Feature\")\n",
    "    #     plt.title(f\"Top {k} Features by F-Regression Score\")\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    #     return scores.head(k)\n",
    "\n",
    "    # def analyze_class_distribution(self, target, n_bins=5, plot=True):\n",
    "    #     \"\"\"\n",
    "    #     Analyze and visualize target distribution with proper binning\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         # Try qcut first\n",
    "    #         try:\n",
    "    #             bins = pd.qcut(\n",
    "    #                 target,\n",
    "    #                 q=n_bins,\n",
    "    #                 labels=[f\"Bin {i+1}\" for i in range(n_bins)],\n",
    "    #                 duplicates=\"drop\",\n",
    "    #             )\n",
    "    #         except ValueError:\n",
    "    #             # Fallback to cut if qcut fails\n",
    "    #             bins = pd.cut(\n",
    "    #                 target, bins=n_bins, labels=[f\"Bin {i+1}\" for i in range(n_bins)]\n",
    "    #             )\n",
    "\n",
    "    #         distribution = Counter(bins)\n",
    "\n",
    "    #         if plot:\n",
    "    #             plt.figure(figsize=(10, 6))\n",
    "    #             sns.countplot(x=bins)\n",
    "    #             plt.title(\"Target Variable Distribution (Plays)\")\n",
    "    #             plt.xlabel(\"Play Count Bins\")\n",
    "    #             plt.ylabel(\"Frequency\")\n",
    "    #             plt.xticks(rotation=45)\n",
    "\n",
    "    #             # Add value labels on bars\n",
    "    #             for i, v in enumerate(distribution.values()):\n",
    "    #                 plt.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    #             plt.tight_layout()\n",
    "    #             plt.show()\n",
    "\n",
    "    #         # Calculate and display statistics\n",
    "    #         max_class = max(distribution.values())\n",
    "    #         min_class = min(distribution.values())\n",
    "    #         imbalance_ratio = max_class / min_class\n",
    "\n",
    "    #         print(\"\\nDistribution Statistics:\")\n",
    "    #         print(f\"Total samples: {len(target)}\")\n",
    "    #         print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    #         print(\"\\nBin Ranges:\")\n",
    "\n",
    "    #         # Get bin edges without accessing categories\n",
    "    #         bin_edges = pd.IntervalIndex(bins.unique()).to_tuples()\n",
    "    #         for i, (lower, upper) in enumerate(bin_edges):\n",
    "    #             print(f\"Bin {i+1}: ({lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "    #         return distribution\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error analyzing distribution: {str(e)}\")\n",
    "    #         return None\n",
    "\n",
    "    # def balance_classes(self, X, y, method=\"smote\", sampling_strategy=\"auto\"):\n",
    "    #     \"\"\"Balance classes using SMOTE or undersampling\"\"\"\n",
    "    #     if method.lower() == \"smote\":\n",
    "    #         sampler = SMOTE(sampling_strategy=sampling_strategy)\n",
    "    #     else:\n",
    "    #         sampler = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "    #     X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    #     return X_resampled, y_resampled\n",
    "\n",
    "    # def inspect_tfidf_features(self, feature_name=\"music\"):\n",
    "    #     \"\"\"Inspect TF-IDF vectorizer features\"\"\"\n",
    "    #     vectorizer = getattr(self, f\"{feature_name}_tfidf_vectorizer\")\n",
    "\n",
    "    #     # Get vocabulary and feature names\n",
    "    #     vocab = vectorizer.vocabulary_\n",
    "    #     feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    #     print(f\"\\n{feature_name.title()} TF-IDF Features:\")\n",
    "    #     print(f\"Total features: {len(feature_names)}\")\n",
    "    #     print(f\"Max features setting: {vectorizer.max_features}\")\n",
    "    #     print(\"\\nTop 10 features by vocabulary index:\")\n",
    "    #     for word, idx in sorted(vocab.items(), key=lambda x: x[1])[:10]:\n",
    "    #         print(f\"{word}: {idx}\")\n",
    "\n",
    "    #     return vocab, feature_names\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "\"\"\"\n",
    "preprocessor = DataPreprocessor()\n",
    "data = preprocessor.load_data(\"your_data.csv\")\n",
    "\n",
    "# Check class distribution\n",
    "distribution = preprocessor.analyze_class_distribution(data['plays'])\n",
    "\n",
    "# Balance classes if needed\n",
    "X_resampled, y_resampled = preprocessor.balance_classes(\n",
    "    data_encoded, \n",
    "    data['plays'],\n",
    "    method='smote'\n",
    ")\n",
    "\n",
    "# Inspect TF-IDF features\n",
    "vocab, features = preprocessor.inspect_tfidf_features('music')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor = DataPreprocessor(\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    max_artist_features=3995,\n",
    "    max_genre_features=21,\n",
    "    max_music_features=5784,\n",
    ")\n",
    "\n",
    "# Load and preprocess data\n",
    "filepath = \"../../data/engineered_data.csv\"\n",
    "\n",
    "# In main function, add error handling for data loading\n",
    "try:\n",
    "    data = preprocessor.load_data(filepath)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>music</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>featured_artists</th>\n",
       "      <th>genre</th>\n",
       "      <th>duration</th>\n",
       "      <th>music_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>explicit</th>\n",
       "      <th>release_year</th>\n",
       "      <th>music_age</th>\n",
       "      <th>plays_log</th>\n",
       "      <th>energy_loudness</th>\n",
       "      <th>dance_valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>F</td>\n",
       "      <td>Bank Account</td>\n",
       "      <td>21 Savage</td>\n",
       "      <td>Birdy, Zoé</td>\n",
       "      <td>Dark Trap</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2fQrGHiQOvpL9UgPvtYy6G</td>\n",
       "      <td>a0b0b79c90af400d012c20ff4e5190d46ea6da7d00a9f4...</td>\n",
       "      <td>-0.955407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428590</td>\n",
       "      <td>-0.934219</td>\n",
       "      <td>-2.537022</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>-0.132277</td>\n",
       "      <td>-0.972830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>F</td>\n",
       "      <td>Little Talks</td>\n",
       "      <td>Of Monsters and Men</td>\n",
       "      <td>Ninho, Snoop Dogg, Russ, Paramore</td>\n",
       "      <td>Other</td>\n",
       "      <td>4.44</td>\n",
       "      <td>2ihCaVdNZmnHZWt0fvAM7B</td>\n",
       "      <td>669122254ed9bfcd862183da62571b3ece680da165f4fb...</td>\n",
       "      <td>-0.385361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428619</td>\n",
       "      <td>1.274342</td>\n",
       "      <td>-1.036430</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>6.532334</td>\n",
       "      <td>-0.109632</td>\n",
       "      <td>-0.047337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>M</td>\n",
       "      <td>Wherever I Go</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>Keith Urban, DJ Khaled, NIKI, MF DOOM</td>\n",
       "      <td>Other</td>\n",
       "      <td>2.83</td>\n",
       "      <td>46jLy47W8rkf8rEX04gMKB</td>\n",
       "      <td>8a1176f697531f10b9f2678fec4c1c85194a3165148d20...</td>\n",
       "      <td>0.242743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250512</td>\n",
       "      <td>1.147169</td>\n",
       "      <td>0.022241</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2009</td>\n",
       "      <td>15</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>0.077155</td>\n",
       "      <td>-0.039519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>No New Friends</td>\n",
       "      <td>DJ Khaled</td>\n",
       "      <td>The xx, LIT killah</td>\n",
       "      <td>Pop</td>\n",
       "      <td>5.14</td>\n",
       "      <td>5oVlbbiKGdGeZkWCFy0mqk</td>\n",
       "      <td>cb25617212d96d7e96e5aadec386fc662d7252337f076c...</td>\n",
       "      <td>1.317553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428619</td>\n",
       "      <td>-0.827931</td>\n",
       "      <td>-0.623654</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>5.442418</td>\n",
       "      <td>-0.267584</td>\n",
       "      <td>-0.607147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>Dreams</td>\n",
       "      <td>Campsite Dream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Country</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1SNoSoQ3JZldOhzBY9gw0n</td>\n",
       "      <td>09207797345aaafddfca55ff08c4be65c0392d8411b175...</td>\n",
       "      <td>0.228950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537232</td>\n",
       "      <td>0.109130</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1974</td>\n",
       "      <td>50</td>\n",
       "      <td>5.971262</td>\n",
       "      <td>0.117367</td>\n",
       "      <td>0.081196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age gender           music          artist_name  \\\n",
       "0   16      F    Bank Account            21 Savage   \n",
       "1   16      F    Little Talks  Of Monsters and Men   \n",
       "2   17      M   Wherever I Go          OneRepublic   \n",
       "3   44      M  No New Friends            DJ Khaled   \n",
       "4   44      M          Dreams       Campsite Dream   \n",
       "\n",
       "                        featured_artists      genre  duration  \\\n",
       "0                             Birdy, Zoé  Dark Trap      3.67   \n",
       "1      Ninho, Snoop Dogg, Russ, Paramore      Other      4.44   \n",
       "2  Keith Urban, DJ Khaled, NIKI, MF DOOM      Other      2.83   \n",
       "3                     The xx, LIT killah        Pop      5.14   \n",
       "4                                    NaN    Country      3.20   \n",
       "\n",
       "                 music_id                                          artist_id  \\\n",
       "0  2fQrGHiQOvpL9UgPvtYy6G  a0b0b79c90af400d012c20ff4e5190d46ea6da7d00a9f4...   \n",
       "1  2ihCaVdNZmnHZWt0fvAM7B  669122254ed9bfcd862183da62571b3ece680da165f4fb...   \n",
       "2  46jLy47W8rkf8rEX04gMKB  8a1176f697531f10b9f2678fec4c1c85194a3165148d20...   \n",
       "3  5oVlbbiKGdGeZkWCFy0mqk  cb25617212d96d7e96e5aadec386fc662d7252337f076c...   \n",
       "4  1SNoSoQ3JZldOhzBY9gw0n  09207797345aaafddfca55ff08c4be65c0392d8411b175...   \n",
       "\n",
       "   acousticness  ...  instrumentalness  liveness     tempo  time_signature  \\\n",
       "0     -0.955407  ...         -0.428590 -0.934219 -2.537022             4.0   \n",
       "1     -0.385361  ...         -0.428619  1.274342 -1.036430             4.0   \n",
       "2      0.242743  ...         -0.250512  1.147169  0.022241             4.0   \n",
       "3      1.317553  ...         -0.428619 -0.827931 -0.623654             4.0   \n",
       "4      0.228950  ...          0.537232  0.109130  0.002027             4.0   \n",
       "\n",
       "   explicit  release_year  music_age  plays_log  energy_loudness  \\\n",
       "0      True          2017          7   2.484907        -0.132277   \n",
       "1     False          2013         11   6.532334        -0.109632   \n",
       "2     False          2009         15   4.919981         0.077155   \n",
       "3     False          2017          7   5.442418        -0.267584   \n",
       "4     False          1974         50   5.971262         0.117367   \n",
       "\n",
       "   dance_valence  \n",
       "0      -0.972830  \n",
       "1      -0.047337  \n",
       "2      -0.039519  \n",
       "3      -0.607147  \n",
       "4       0.081196  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = preprocessor.load_data(filepath)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate plays by song for each user\n",
    "# import pandas as pd\n",
    "# from io import StringIO\n",
    "\n",
    "# # Reading data into a DataFrame\n",
    "# df = data.copy()\n",
    "\n",
    "# # Aggregating plays by song\n",
    "# aggregated_plays = df.groupby(\"music\")[\"plays\"].sum().reset_index()\n",
    "\n",
    "# # Output the result\n",
    "# print(aggregated_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = DataPreprocessor()\n",
    "# filepath = \"../data/cleaned_modv2.csv\"\n",
    "# data = preprocessor.load_data(filepath)\n",
    "# data_encoded = preprocessor.encode_features_train(data)\n",
    "# top_features = preprocessor.analyze_feature_importance(\n",
    "#     data_encoded, data[\"plays\"], k=20\n",
    "# )\n",
    "# print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = DataPreprocessor()\n",
    "# data = preprocessor.load_data(filepath)\n",
    "\n",
    "# # Encode features\n",
    "# data_encoded = preprocessor.encode_features_train(data)\n",
    "\n",
    "# # Check class distribution\n",
    "# distribution = preprocessor.analyze_class_distribution(data[\"plays\"])\n",
    "\n",
    "# # Balance classes if needed\n",
    "# X_resampled, y_resampled = preprocessor.balance_classes(\n",
    "#     data_encoded, data[\"plays\"], method=\"smote\"\n",
    "# )\n",
    "\n",
    "# # Inspect TF-IDF features\n",
    "# vocab, features = preprocessor.inspect_tfidf_features(\"music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution = preprocessor.analyze_class_distribution(data[\"plays\"], n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab, features = preprocessor.inspect_tfidf_features(\"music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inspect all tfidf features\n",
    "# feature_names = [\"genre\", \"music\", \"artist\"]\n",
    "# vocab_features = [\n",
    "#     preprocessor.inspect_tfidf_features(feature_name) for feature_name in feature_names\n",
    "# ]\n",
    "# vocab, features = zip(*vocab_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EnhancedListNetLoss(nn.Module):\n",
    "    def __init__(self, k=10, ils_weight=0.1, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Enhanced ListNet Loss with Intra-List Similarity regularization\n",
    "\n",
    "        Args:\n",
    "            k (int): Top-k items to consider\n",
    "            ils_weight (float): Weight for the ILS regularization term\n",
    "            temperature (float): Temperature for similarity scaling\n",
    "        \"\"\"\n",
    "        super(EnhancedListNetLoss, self).__init__()\n",
    "        self.k = k\n",
    "        self.ils_weight = ils_weight\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_similarity_matrix(self, features):\n",
    "        \"\"\"\n",
    "        Compute pairwise similarities between items in the batch\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): Combined feature representation [batch_size, feature_dim]\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        normalized_features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        similarity_matrix = torch.mm(normalized_features, normalized_features.t())\n",
    "\n",
    "        # Scale similarities by temperature\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def compute_ils_penalty(self, similarity_matrix, rankings):\n",
    "        \"\"\"\n",
    "        Compute ILS penalty based on item similarities and their positions in ranking\n",
    "\n",
    "        Args:\n",
    "            similarity_matrix (torch.Tensor): Pairwise similarity matrix [batch_size, batch_size]\n",
    "            rankings (torch.Tensor): Predicted rankings [batch_size, 1]\n",
    "        \"\"\"\n",
    "        batch_size = rankings.size(0)\n",
    "\n",
    "        # Convert rankings to pairwise position differences\n",
    "        position_diff = (rankings - rankings.t()).abs()\n",
    "\n",
    "        # Weight similarities by position differences (closer positions = higher penalty)\n",
    "        position_weights = torch.exp(-position_diff)\n",
    "\n",
    "        # Compute penalty: high similarity items should be far apart in ranking\n",
    "        ils_penalty = (similarity_matrix * position_weights).sum() / (\n",
    "            batch_size * (batch_size - 1)\n",
    "        )\n",
    "\n",
    "        return ils_penalty\n",
    "\n",
    "    def combine_features(self, genre_features, artist_features, music_features):\n",
    "        \"\"\"\n",
    "        Combine different feature types with appropriate weighting\n",
    "        \"\"\"\n",
    "        # Normalize each feature type\n",
    "        genre_norm = F.normalize(genre_features, p=2, dim=1)\n",
    "        artist_norm = F.normalize(artist_features, p=2, dim=1)\n",
    "        music_norm = F.normalize(music_features, p=2, dim=1)\n",
    "\n",
    "        # Combine features with weights\n",
    "        # You can adjust these weights based on importance\n",
    "        combined = torch.cat(\n",
    "            [\n",
    "                genre_norm * 0.4,  # Higher weight for genre diversity\n",
    "                artist_norm * 0.3,\n",
    "                music_norm * 0.3,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return combined\n",
    "\n",
    "    def forward(self, y_pred, y_true, genre_features, artist_features, music_features):\n",
    "        \"\"\"\n",
    "        Forward pass computing both ListNet loss and ILS regularization\n",
    "\n",
    "        Args:\n",
    "            y_pred (torch.Tensor): Predicted scores [batch_size, 1]\n",
    "            y_true (torch.Tensor): True scores [batch_size, 1]\n",
    "            genre_features (torch.Tensor): Genre TF-IDF features\n",
    "            artist_features (torch.Tensor): Artist TF-IDF features\n",
    "            music_features (torch.Tensor): Music TF-IDF features\n",
    "        \"\"\"\n",
    "        # Original ListNet loss\n",
    "        P_y_pred = F.softmax(y_pred, dim=0)\n",
    "        P_y_true = F.softmax(y_true, dim=0)\n",
    "        listnet_loss = -torch.sum(P_y_true * torch.log(P_y_pred + 1e-10)) / y_pred.size(\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # Compute ILS penalty\n",
    "        combined_features = self.combine_features(\n",
    "            genre_features, artist_features, music_features\n",
    "        )\n",
    "        similarity_matrix = self.compute_similarity_matrix(combined_features)\n",
    "        ils_penalty = self.compute_ils_penalty(similarity_matrix, y_pred)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = listnet_loss + self.ils_weight * ils_penalty\n",
    "\n",
    "        return total_loss, {\n",
    "            \"listnet_loss\": listnet_loss.item(),\n",
    "            \"ils_penalty\": ils_penalty.item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "        }\n",
    "\n",
    "    def get_diversity_metric(\n",
    "        self, genre_features, artist_features, music_features, y_pred, k=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute diversity metric for top-k recommendations\n",
    "        \"\"\"\n",
    "        combined_features = self.combine_features(\n",
    "            genre_features, artist_features, music_features\n",
    "        )\n",
    "        similarity_matrix = self.compute_similarity_matrix(combined_features)\n",
    "\n",
    "        # Get top-k indices\n",
    "        _, top_k_indices = torch.topk(y_pred, k=min(k, y_pred.size(0)))\n",
    "\n",
    "        # Compute average similarity between top-k items (lower is more diverse)\n",
    "        top_k_similarities = similarity_matrix[top_k_indices][:, top_k_indices]\n",
    "        diversity_score = 1.0 - (top_k_similarities.sum() - k) / (k * (k - 1))\n",
    "\n",
    "        return diversity_score.item()\n",
    "\n",
    "\n",
    "class HybridRecommender(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_genders,\n",
    "        num_music_items,\n",
    "        num_genres,\n",
    "        num_artist_features,\n",
    "        num_numerical_features,\n",
    "        num_release_years,\n",
    "        embedding_dim,\n",
    "        hidden_dims = [256, 128, 64, 32],\n",
    "        dropout_prob=0.3,\n",
    "    ):\n",
    "        super(HybridRecommender, self).__init__()\n",
    "    \n",
    "        # Embeddings\n",
    "        self.gender_embedding = nn.Embedding(num_genders, embedding_dim)\n",
    "        self.release_year_embedding = nn.Embedding(num_release_years, embedding_dim)\n",
    "\n",
    "        # Feature transformations with batch norm\n",
    "        self.music_fc = nn.Sequential(\n",
    "            nn.Linear(num_music_items, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.genre_fc = nn.Sequential(\n",
    "            nn.Linear(num_genres, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.artist_fc = nn.Sequential(\n",
    "            nn.Linear(num_artist_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Calculate expected dimension\n",
    "        self.expected_dim = 32 + embedding_dim * 5 + num_numerical_features\n",
    "\n",
    "        # Input normalization\n",
    "        self.input_bn = nn.BatchNorm1d(self.expected_dim)\n",
    "\n",
    "        # Main network layers with residual connections\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(self.expected_dim, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.BatchNorm1d(hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob * 0.8),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.BatchNorm1d(hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob * 0.6),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[2], hidden_dims[3]),\n",
    "            nn.BatchNorm1d(hidden_dims[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob * 0.4),\n",
    "        )\n",
    "\n",
    "        # Residual connections\n",
    "        self.res1 = nn.Linear(self.expected_dim, hidden_dims[1])\n",
    "        self.res2 = nn.Linear(hidden_dims[0], hidden_dims[2])\n",
    "        self.res3 = nn.Linear(hidden_dims[1], hidden_dims[3])\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(nn.Linear(hidden_dims[3], 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        user_id_hashed,\n",
    "        artist_features,\n",
    "        gender_id,\n",
    "        music_features,\n",
    "        genre_features,\n",
    "        numerical_features,\n",
    "        release_year,\n",
    "    ):\n",
    "        # Ensure all inputs are 2D\n",
    "        if user_id_hashed.dim() == 1:\n",
    "            user_id_hashed = user_id_hashed.unsqueeze(1)\n",
    "        if gender_id.dim() == 1:\n",
    "            gender_id = gender_id.unsqueeze(1)\n",
    "        if release_year.dim() == 1:\n",
    "            release_year = release_year.unsqueeze(1)\n",
    "        if numerical_features.dim() == 1:\n",
    "            numerical_features = numerical_features.unsqueeze(1)\n",
    "\n",
    "        # Process embeddings\n",
    "        gender_embedded = self.gender_embedding(gender_id.long().squeeze(-1))\n",
    "        release_year_embedded = self.release_year_embedding(release_year.long().squeeze(-1))\n",
    "        \n",
    "        # Process features through FC layers\n",
    "        music_embedded = self.music_fc(music_features.float())\n",
    "        artist_embedded = self.artist_fc(artist_features.float())\n",
    "        genre_embedded = self.genre_fc(genre_features.float())\n",
    "\n",
    "        # Concatenate all features\n",
    "        concat_features = torch.cat(\n",
    "            [\n",
    "                user_id_hashed.float(),\n",
    "                gender_embedded,\n",
    "                release_year_embedded,\n",
    "                music_embedded,\n",
    "                artist_embedded,\n",
    "                genre_embedded,\n",
    "                numerical_features.float(),\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # Add dimension checks\n",
    "        expected_batch_size = user_id_hashed.size(0)\n",
    "        assert all(x.size(0) == expected_batch_size for x in [\n",
    "            gender_embedded, release_year_embedded, music_embedded,\n",
    "            artist_embedded, genre_embedded, numerical_features\n",
    "        ]), \"Batch size mismatch in features\"\n",
    "        \n",
    "        assert concat_features.shape[1] == self.expected_dim, \\\n",
    "            f\"Expected {self.expected_dim} features but got {concat_features.shape[1]}\"\n",
    "\n",
    "        # Rest of the forward pass remains the same\n",
    "        x = self.input_bn(concat_features)\n",
    "        x1 = self.layer1(x)\n",
    "        r1 = self.res1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x2 = x2 + r1\n",
    "        x3 = self.layer3(x2)\n",
    "        r2 = self.res2(x1)\n",
    "        x3 = x3 + r2\n",
    "        x4 = self.layer4(x3)\n",
    "        r3 = self.res3(x2)\n",
    "        x4 = x4 + r3\n",
    "        \n",
    "        return self.output(x4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Error during training: \"['danceability', 'energy', 'loudness', 'valence'] not in index\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['danceability', 'energy', 'loudness', 'valence'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 567\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(ndcg_scores) \u001b[38;5;28;01mif\u001b[39;00m ndcg_scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(precision_scores) \u001b[38;5;28;01mif\u001b[39;00m precision_scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(recall_scores) \u001b[38;5;28;01mif\u001b[39;00m recall_scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(f1_scores) \u001b[38;5;28;01mif\u001b[39;00m f1_scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    564\u001b[0m     }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 264\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplays\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in input data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m data_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_features_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Add plays column to encoded data if not present\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data_encoded\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/Lhydra_rs/data_engineered/rs_main_v2_refactored/preprocessing.py:179\u001b[0m, in \u001b[0;36mDataPreprocessor.encode_features_train\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    147\u001b[0m numerical_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# \"age\",\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m ]\n\u001b[1;32m    165\u001b[0m data_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    166\u001b[0m     {\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id_hashed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(user_id_hashed),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m     }\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m data_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    174\u001b[0m     [\n\u001b[1;32m    175\u001b[0m         data_encoded,\n\u001b[1;32m    176\u001b[0m         music_tfidf_df,\n\u001b[1;32m    177\u001b[0m         artist_tfidf_df,\n\u001b[1;32m    178\u001b[0m         genre_tfidf_df,\n\u001b[0;32m--> 179\u001b[0m         \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_features\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    180\u001b[0m     ],\n\u001b[1;32m    181\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Verify release_year exists\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelease_year\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/anaconda3/envs/ReNeLLM/lib/python3.9/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ReNeLLM/lib/python3.9/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ReNeLLM/lib/python3.9/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['danceability', 'energy', 'loudness', 'valence'] not in index\""
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from preprocessing import DataPreprocessor\n",
    "from model import HybridRecommender, EnhancedListNetLoss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add these near the top of the file, after imports\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "EMBEDDING_DIM = 64\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 0.01\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "EARLY_STOPPING_MIN_DELTA = 0.001\n",
    "SCHEDULER_PATIENCE = 2\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "SCHEDULER_MIN_LR = 1e-6\n",
    "L2_LAMBDA = 1e-4\n",
    "\n",
    "# Add at the top with other constants\n",
    "NUMERICAL_COLS = [\n",
    "        \"age\",\n",
    "        \"duration\",\n",
    "        \"acousticness\",\n",
    "        # \"danceability\",\n",
    "        # \"energy\",\n",
    "        \"key\",\n",
    "        # \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        # \"valence\",\n",
    "        \"tempo\",\n",
    "        \"time_signature\",\n",
    "        \"explicit\",\n",
    "        \"dance_valence\",\n",
    "        \"energy_loudness\",\n",
    "]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001, save_path=\"models/best_model.pth\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")  # Initialize with infinity\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:  # Changed condition\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save model when validation loss decreases.\"\"\"\n",
    "        torch.save(model.state_dict(), self.save_path)\n",
    "\n",
    "\n",
    "def prepare_tensor_data(features, target, device):\n",
    "    \"\"\"Helper function to convert features to tensors.\"\"\"\n",
    "    numerical_cols = [\n",
    "        \"age\",\n",
    "        \"duration\",\n",
    "        \"acousticness\",\n",
    "        # \"danceability\",\n",
    "        # \"energy\",\n",
    "        \"key\",\n",
    "        # \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        # \"valence\",\n",
    "        \"tempo\",\n",
    "        \"time_signature\",\n",
    "        \"explicit\",\n",
    "        \"dance_valence\",\n",
    "        \"energy_loudness\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"user_id\": torch.tensor(\n",
    "            np.stack(features[\"user_id\"].tolist()), dtype=torch.float\n",
    "        ).to(device),\n",
    "        \"gender_ids\": torch.tensor(\n",
    "            features[\"gender_encoded\"].values, dtype=torch.long\n",
    "        ).to(device),\n",
    "        \"genre_features\": torch.tensor(\n",
    "            features[\n",
    "                [col for col in features.columns if col.startswith(\"genre_tfidf_\")]\n",
    "            ].values,\n",
    "            dtype=torch.float,\n",
    "        ).to(device),\n",
    "        \"artist_features\": torch.tensor(\n",
    "            features[\n",
    "                [col for col in features.columns if col.startswith(\"artist_tfidf_\")]\n",
    "            ].values,\n",
    "            dtype=torch.float,\n",
    "        ).to(device),\n",
    "        \"music_features\": torch.tensor(\n",
    "            features[\n",
    "                [col for col in features.columns if col.startswith(\"music_tfidf_\")]\n",
    "            ].values,\n",
    "            dtype=torch.float,\n",
    "        ).to(device),\n",
    "        \"numerical_features\": torch.tensor(\n",
    "            features[numerical_cols].values, dtype=torch.float\n",
    "        ).to(device),\n",
    "        \"release_years\": torch.tensor(\n",
    "            features[\"release_year_encoded\"].values, dtype=torch.long\n",
    "        ).to(device),\n",
    "        \"target\": torch.tensor(target.values, dtype=torch.float)\n",
    "        .unsqueeze(1)\n",
    "        .to(device),\n",
    "    }\n",
    "\n",
    "\n",
    "def create_dataloader(tensor_data, batch_size=128, shuffle=True):\n",
    "    \"\"\"Helper function to create a DataLoader from tensor data.\"\"\"\n",
    "    try:\n",
    "        dataset = TensorDataset(\n",
    "            tensor_data[\"user_id\"],\n",
    "            tensor_data[\"artist_features\"],\n",
    "            tensor_data[\"gender_ids\"],\n",
    "            tensor_data[\"music_features\"],\n",
    "            tensor_data[\"genre_features\"],\n",
    "            tensor_data[\"numerical_features\"],\n",
    "            tensor_data[\"release_years\"],\n",
    "            tensor_data[\"target\"],\n",
    "        )\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataLoader: {e}\")\n",
    "        print(\"Tensor shapes:\")\n",
    "        for key, tensor in tensor_data.items():\n",
    "            print(f\"{key}: {tensor.shape}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            (\n",
    "                batch_user_id_hashed,\n",
    "                batch_artist_features,\n",
    "                batch_gender_ids,\n",
    "                batch_music_features,\n",
    "                batch_genre_features,\n",
    "                batch_numerical_features,\n",
    "                batch_release_years,\n",
    "                batch_target,\n",
    "            ) = batch\n",
    "\n",
    "            predictions = model(\n",
    "                batch_user_id_hashed,\n",
    "                batch_artist_features,\n",
    "                batch_gender_ids,\n",
    "                batch_music_features,\n",
    "                batch_genre_features,\n",
    "                batch_numerical_features,\n",
    "                batch_release_years,\n",
    "            )\n",
    "            batch_loss, _ = criterion(\n",
    "                predictions,\n",
    "                batch_target,\n",
    "                batch_genre_features,\n",
    "                batch_artist_features,\n",
    "                batch_music_features,\n",
    "            )\n",
    "            val_loss += batch_loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        (\n",
    "            batch_user_id_hashed,\n",
    "            batch_artist_features,\n",
    "            batch_gender_ids,\n",
    "            batch_music_features,\n",
    "            batch_genre_features,\n",
    "            batch_numerical_features,\n",
    "            batch_release_years,\n",
    "            batch_target,\n",
    "        ) = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(\n",
    "            batch_user_id_hashed,\n",
    "            batch_artist_features,\n",
    "            batch_gender_ids,\n",
    "            batch_music_features,\n",
    "            batch_genre_features,\n",
    "            batch_numerical_features,\n",
    "            batch_release_years,\n",
    "        )\n",
    "\n",
    "        loss, loss_details = criterion(\n",
    "            predictions,\n",
    "            batch_target,\n",
    "            batch_genre_features,\n",
    "            batch_artist_features,\n",
    "            batch_music_features,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Optionally, log loss details\n",
    "        print(\n",
    "            f\"ListNet Loss: {loss_details['listnet_loss']}, ILS Penalty: {loss_details['ils_penalty']}, Total Loss: {loss_details['total_loss']}\"\n",
    "        )\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Define device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Initialize the preprocessor\n",
    "        preprocessor = DataPreprocessor(\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            max_artist_features=3995,\n",
    "            max_genre_features=21,\n",
    "            max_music_features=5784,\n",
    "        )\n",
    "\n",
    "        # Load and preprocess data\n",
    "        filepath = \"../../data/engineered_data.csv\"\n",
    "\n",
    "        # In main function, add error handling for data loading\n",
    "        try:\n",
    "            data = preprocessor.load_data(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Loaded data is empty\")\n",
    "\n",
    "        # Ensure 'plays' column exists\n",
    "        if \"plays\" not in data.columns:\n",
    "            raise ValueError(\"Target column 'plays' not found in input data\")\n",
    "\n",
    "        data_encoded = preprocessor.encode_features_train(data)\n",
    "\n",
    "        # Add plays column to encoded data if not present\n",
    "        if \"plays\" not in data_encoded.columns:\n",
    "            data_encoded[\"plays\"] = data[\"plays\"]\n",
    "\n",
    "        features = preprocessor.feature_engineering(data_encoded)\n",
    "\n",
    "        # Verify plays column exists before splitting\n",
    "        if \"plays\" not in features.columns:\n",
    "            raise ValueError(\"Target column 'plays' lost during preprocessing\")\n",
    "\n",
    "        # Correctly define music_feature_cols before splitting\n",
    "        music_feature_cols = [\n",
    "            col for col in features.columns if col.startswith(\"music_tfidf_\")\n",
    "        ]\n",
    "\n",
    "        (\n",
    "            train_features,\n",
    "            val_features,\n",
    "            test_features,\n",
    "            train_target,\n",
    "            val_target,\n",
    "            test_target,\n",
    "        ) = preprocessor.split_data(features)\n",
    "        # Save preprocessors\n",
    "        preprocessor.save_preprocessors(directory=\"models/\")\n",
    "\n",
    "        # Convert features to tensors\n",
    "        train_tensors = prepare_tensor_data(train_features, train_target, device)\n",
    "        val_tensors = prepare_tensor_data(val_features, val_target, device)\n",
    "        test_tensors = prepare_tensor_data(test_features, test_target, device)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_dataloader = create_dataloader(\n",
    "            train_tensors, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",
    "        )\n",
    "        val_dataloader = create_dataloader(\n",
    "            val_tensors, batch_size=VAL_BATCH_SIZE, shuffle=False\n",
    "        )\n",
    "\n",
    "        # Add this debug code before model initialization:\n",
    "        print(\"Feature dimensions:\")\n",
    "        print(f\"Gender classes: {len(preprocessor.gender_encoder.classes_)}\")\n",
    "        print(\n",
    "            f\"Music TF-IDF features: {len([col for col in train_features.columns if col.startswith('music_tfidf_')])}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Genre TF-IDF features: {len([col for col in train_features.columns if col.startswith('genre_tfidf_')])}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Artist TF-IDF features: {len([col for col in train_features.columns if col.startswith('artist_tfidf_')])}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Release year classes: {len(preprocessor.release_year_encoder.categories_[0])}\"\n",
    "        )\n",
    "\n",
    "        # Add this debug code in train.py before model initialization\n",
    "        music_feature_cols = [\n",
    "            col for col in train_features.columns if col.startswith(\"music_tfidf_\")\n",
    "        ]\n",
    "        print(f\"Actual music features count: {len(music_feature_cols)}\")\n",
    "        print(f\"First few music feature names: {music_feature_cols[:5]}\")\n",
    "\n",
    "        # Before model initialization, add these variables\n",
    "        num_genres = len(\n",
    "            [col for col in train_features.columns if col.startswith(\"genre_tfidf_\")]\n",
    "        )\n",
    "        num_artist_features = len(\n",
    "            [col for col in train_features.columns if col.startswith(\"artist_tfidf_\")]\n",
    "        )\n",
    "\n",
    "        # Update model initialization\n",
    "        model = HybridRecommender(\n",
    "            num_genders=len(preprocessor.gender_encoder.classes_),\n",
    "            num_music_items=len(music_feature_cols),\n",
    "            num_genres=num_genres,\n",
    "            num_artist_features=num_artist_features,\n",
    "            num_numerical_features=15,\n",
    "            num_release_years=len(preprocessor.release_year_encoder.categories_[0]),\n",
    "            embedding_dim=64,\n",
    "        ).to(device)\n",
    "\n",
    "        # After model initialization\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "        # Define optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=SCHEDULER_FACTOR,\n",
    "            patience=SCHEDULER_PATIENCE,\n",
    "            min_lr=SCHEDULER_MIN_LR,\n",
    "        )\n",
    "\n",
    "        # Define loss function\n",
    "        criterion = EnhancedListNetLoss(k=10)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            min_delta=EARLY_STOPPING_MIN_DELTA,\n",
    "            save_path=\"models/best_model.pth\",\n",
    "        )\n",
    "        val_losses = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Training phase\n",
    "            epoch_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = validate_model(model, val_dataloader, criterion, device)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Early Stopping check\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {epoch_loss/len(train_dataloader):.4f}, Val Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Load the best model before evaluation\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(\"models/best_model.pth\"))\n",
    "            model = model.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load best model: {e}\")\n",
    "            print(\"Using last model state instead\")\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(\n",
    "                test_tensors[\"user_id\"],\n",
    "                test_tensors[\"artist_features\"],\n",
    "                test_tensors[\"gender_ids\"],\n",
    "                test_tensors[\"music_features\"],\n",
    "                test_tensors[\"genre_features\"],\n",
    "                test_tensors[\"numerical_features\"],\n",
    "                test_tensors[\"release_years\"],\n",
    "            )\n",
    "\n",
    "        # Convert tensors to numpy arrays\n",
    "        predictions_np = predictions.cpu().numpy().reshape(-1)\n",
    "        test_target_np = test_tensors[\"target\"].cpu().numpy().reshape(-1)\n",
    "        test_user_ids = test_features[\"user_id\"].values\n",
    "\n",
    "        # Evaluate the model per user\n",
    "        evaluate_model_per_user(predictions_np, test_target_np, test_user_ids, k=10)\n",
    "\n",
    "        # Save the trained model\n",
    "        torch.save(model.state_dict(), \"models/model.pth\")\n",
    "        print(\"Model saved to 'models/model.pth'\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        # Add these print statements at key points\n",
    "        print(f\"Data loaded successfully. Shape: {data.shape}\")\n",
    "        print(f\"Training samples: {len(train_dataloader.dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataloader.dataset)}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        # Save the model in its current state\n",
    "        torch.save(model.state_dict(), \"models/interrupted_model.pth\")\n",
    "        print(\"Model saved to 'models/interrupted_model.pth'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Modified NDCG@k implementation.\"\"\"\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    # Get predicted ranking\n",
    "    pred_indices = np.argsort(y_pred)[::-1][:k]\n",
    "\n",
    "    # Calculate DCG\n",
    "    dcg = np.sum([y_true[idx] / np.log2(i + 2) for i, idx in enumerate(pred_indices)])\n",
    "\n",
    "    # Calculate ideal DCG\n",
    "    ideal_indices = np.argsort(y_true)[::-1][:k]\n",
    "    idcg = np.sum([y_true[idx] / np.log2(i + 2) for i, idx in enumerate(ideal_indices)])\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Modified Precision@k implementation.\"\"\"\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    # Get top k indices\n",
    "    indices = np.argsort(y_pred)[::-1][:k]\n",
    "\n",
    "    # Get actual top k indices\n",
    "    actual_top_k = np.argsort(y_true)[::-1][:k]\n",
    "\n",
    "    # Calculate precision\n",
    "    true_positives = len(set(indices) & set(actual_top_k))\n",
    "    return true_positives / k\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Modified Recall@k implementation.\"\"\"\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    # Get top k indices\n",
    "    indices = np.argsort(y_pred)[::-1][:k]\n",
    "\n",
    "    # Get actual top k indices\n",
    "    actual_top_k = np.argsort(y_true)[::-1][:k]\n",
    "\n",
    "    # Calculate recall\n",
    "    true_positives = len(set(indices) & set(actual_top_k))\n",
    "    return true_positives / len(actual_top_k)\n",
    "\n",
    "def f1_score_at_k(y_true, y_pred, k):\n",
    "    \"\"\"Modified F1@k implementation.\"\"\"\n",
    "    precision = precision_at_k(y_true, y_pred, k)\n",
    "    recall = recall_at_k(y_true, y_pred, k)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate_model_per_user(predictions, test_targets, user_ids, k=10):\n",
    "    \"\"\"Evaluate model performance per user using ranking metrics.\"\"\"\n",
    "    # Convert user_ids from list of numpy arrays to tuple of values for hashing\n",
    "    try:\n",
    "        # Convert each numpy array to a tuple for hashing\n",
    "        user_ids_hashable = [tuple(uid) for uid in user_ids]\n",
    "\n",
    "        # Get unique users\n",
    "        unique_users = list(set(map(tuple, user_ids)))\n",
    "\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "\n",
    "        for user in unique_users:\n",
    "            # Create mask for current user\n",
    "            user_mask = [tuple(uid) == user for uid in user_ids_hashable]\n",
    "            user_mask = np.array(user_mask)\n",
    "\n",
    "            user_pred = predictions[user_mask]\n",
    "            user_true = test_targets[user_mask]\n",
    "\n",
    "            if len(user_pred) > 0:\n",
    "                relevance_threshold = np.percentile(user_true, 80)\n",
    "                user_true_binary = (user_true >= relevance_threshold).astype(int)\n",
    "\n",
    "                ndcg_scores.append(ndcg_at_k(user_true, user_pred, k))\n",
    "                precision_scores.append(precision_at_k(user_true_binary, user_pred, k))\n",
    "                recall_scores.append(recall_at_k(user_true_binary, user_pred, k))\n",
    "                f1_scores.append(f1_score_at_k(user_true_binary, user_pred, k))\n",
    "\n",
    "        # Calculate and print averages\n",
    "        if ndcg_scores:\n",
    "            print(f\"Average NDCG@{k}: {np.mean(ndcg_scores):.4f}\")\n",
    "            print(f\"Average Precision@{k}: {np.mean(precision_scores):.4f}\")\n",
    "            print(f\"Average Recall@{k}: {np.mean(recall_scores):.4f}\")\n",
    "            print(f\"Average F1-score@{k}: {np.mean(f1_scores):.4f}\")\n",
    "        else:\n",
    "            print(\"No valid predictions found for evaluation\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        print(f\"user_ids shape: {user_ids.shape}, dtype: {user_ids.dtype}\")\n",
    "        print(f\"predictions shape: {predictions.shape}, dtype: {predictions.dtype}\")\n",
    "        print(f\"test_targets shape: {test_targets.shape}, dtype: {test_targets.dtype}\")\n",
    "        raise\n",
    "\n",
    "    return {\n",
    "        \"ndcg\": np.mean(ndcg_scores) if ndcg_scores else 0,\n",
    "        \"precision\": np.mean(precision_scores) if precision_scores else 0,\n",
    "        \"recall\": np.mean(recall_scores) if recall_scores else 0,\n",
    "        \"f1\": np.mean(f1_scores) if f1_scores else 0,\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
